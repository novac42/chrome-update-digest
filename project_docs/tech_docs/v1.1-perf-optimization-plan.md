# Enhanced WebPlatform Digest Tool - v1.1 Performance Optimization Plan

## 背景
- 基于 `project_docs/tech_docs/1.0-architecture-review-webplatform-digest.md` 的评审结果，当前实现存在并发调度粗糙、I/O 阻塞、缓存策略缺失等性能瓶颈。
- 目标是在不牺牲功能正确性的前提下，缩短单次 digest 生成时长、降低资源占用，并为后续扩展提供可观测与治理能力。

## 目标
- 将单区域 digest 生成的 P95 时长降低 30%。
- 将同一版本全量生成的峰值内存占用下降 20%。
- 建立基础性能指标采集与告警能力，覆盖 LLM 调用、文件 I/O、并发任务执行。
- 在 MCP 层实现工具实例复用与互斥控制，避免跨工具抢占资源。

## 里程碑
1. **M1 - Telemetry & Baseline — 已完成**: 完成性能基线采集与指标埋点。
2. **M2 - Concurrency Governance — 已完成**: 引入统一的并发与速率控制，优化运行时调度。
3. **M3 - Pipeline Efficiency — 进行中**: 精简 pipeline 结构，减少重复 I/O 与内存占用。
4. **M4 - MCP Runtime Coordination — 待开始**: 在 FastMCP 层落实实例生命周期与共享资源治理。

## 任务清单

### M1 - Telemetry & Baseline（已完成）
- ✅ **T1.1 基线框架**：已引入统一 `DigestTelemetry`（Prometheus 指标 + JSONL 事件），落地在 `.monitoring/webplatform-telemetry.jsonl`；形成 per-area 运行上下文（版本、channel、语言、区域数、总时长）。  
  `新增: src/utils/telemetry.py`
- ✅ **T1.2 阶段耗时指标**：在 `_generate_per_area_digests` 和 `_safe_sample_with_retry` 中埋点 `english_generation`、`translation`、`fallback_generation`、`translation_fallback`、`area_total` 等阶段耗时与状态；并对进度写入做节流以减少 I/O 抖动。  
  `改动: src/mcp_tools/enhanced_webplatform_digest.py`
- ✅ **T1.3 错误/重试计数**：对 LLM 调用的每次尝试记录 success/timeout/error 与 attempt；统计重试计数；对校验失败与异常进行错误计数与事件落盘。  
  `改动: src/mcp_tools/enhanced_webplatform_digest.py`

### M2 - Concurrency Governance（已完成）
- ✅ **T2.1 并发控制器**：集中配置并发与重试参数，新增环境变量：
  - `WEBPLATFORM_MAX_CONCURRENCY`（默认 4）
  - `WEBPLATFORM_RATE_LIMIT` 每秒调用数（默认 2）
  - `WEBPLATFORM_FAILURE_COOLDOWN` 熔断冷却秒数（默认 5）
  - `WEBPLATFORM_CIRCUIT_THRESHOLD` 连续失败阈值（默认 3）
  已在 `__init__` 中初始化统一控制，并在采样路径中以 `Semaphore`、速率间隔与冷却窗口生效。
  `改动: src/mcp_tools/enhanced_webplatform_digest.py`
- ✅ **T2.2 进度写入节流**：`_update_progress` 已实现基于 `perf_counter` 的时间间隔与“完成计数不变则跳过”策略；可通过 `WEBPLATFORM_PROGRESS_MIN_INTERVAL` 配置（默认 0.5s）。
  `确认: src/mcp_tools/enhanced_webplatform_digest.py`
- ✅ **T2.3 速率限制与熔断**：在 `_safe_sample_with_retry` 中加入：
  - 基于 `Semaphore` 的并发闸门
  - 简单令牌间隔速率限制
  - 连续失败阈值 + 冷却期的熔断控制
  并配合 Telemetry 记录 `success/timeout/error`。  
  `改动: src/mcp_tools/enhanced_webplatform_digest.py`

### M3 - Pipeline Efficiency（进行中）
- ✅（骨架）**T3.1 `process_area` 抽取**：已新增 `AreaRunner.process_one_area(...)` 骨架，暂保持原行为，由 `_generate_per_area_digests` 调度；后续将逐步把内联逻辑迁移到执行器内。  
  `新增: src/mcp_tools/_digest_area_runner.py`  
  `参考: src/mcp_tools/enhanced_webplatform_digest.py:1168-1700`
  - 进展：已按“最小替换”将空区域（无 features）fallback 分支改由 `AreaRunner` 负责生成与写入，继续沿用原 Telemetry 与进度写入时序（字段不变）。
- ✅ **T3.2 YAML 加载与缓存**：已加入进程内 YAML 缓存层，内存命中优先；磁盘命中后回填内存缓存，并统计命中/未命中计数；首轮生成后对区域 YAML 进行回填。  
  `改动: src/mcp_tools/enhanced_webplatform_digest.py:44, 205-360`  
  `新增: _yaml_cache/_yaml_cache_hits/_yaml_cache_misses；_get_yaml_data 中的 cache_key/memory-first 逻辑`
- ⏳ **T3.3 模型偏好作用域**：限制 `_model_preferences` 仅在单次 run 生命周期内有效，后续补充 run 结束清理或按 ctx 粒度缓存，避免实例级泄漏。  
  `参考: src/mcp_tools/enhanced_webplatform_digest.py:62-107, 146-168`
- ✅ **T3.4 写入路径统一**：合并 EN/ZH 写入的公共流程（路径计算、保存、进度与 telemetry 更新），减少重复 I/O 与事件写入。  
  `新增: src/mcp_tools/_digest_io.py`  
  `改动: src/mcp_tools/enhanced_webplatform_digest.py, src/mcp_tools/_digest_area_runner.py`

整合的结构化重构计划（与 1.0 评审对齐）
- 阶段一（已开始）
  - IO/缓存门面最小抽离：先在原类内实现内存缓存（已完成 T3.2），后续逐步下沉到 `src/mcp_tools/_digest_yaml_cache.py` 与 `_digest_io.py`。
  - 保持对外接口与产物路径不变，便于回归。
- 阶段二（进行中）
- 抽出 YAML Pipeline 门面 `_digest_yaml_pipeline.py`，统一 `load/process/aggregate/load_area` 接口，减少工具类分支复杂度。
- 抽出 per-area 执行器 `_digest_area_runner.py`，`_generate_per_area_digests` 仅负责调度与汇总（对应 T3.1，骨架已落地）。
- 抽出 I/O 与进度门面 `_digest_io.py`，统一文本持久化与 telemetry/进度更新，支撑 T3.4。
- 阶段三（计划）
  - 拆分生成/翻译引擎 `_digest_generation.py`，将英文生成/校验/重试与翻译/校验/重试及 fallback 细化为纯函数，强化可测试性（支撑 T3.4）。
  - 模型偏好与运行参数集中于 `_digest_config.py`，解决状态泄漏（完成 T3.3）。

### M4 - MCP Split and Runtime Coordination (partly, the rest is deprioritized) ✅
- #### MCP Tool Split Plan
- Pipeline 工具：`digest.prepare_yaml`（预热缓存并枚举区域）、`digest.generate_area`（单区域英文生成 + 缓存写入）、`digest.translate_area`（按需触发翻译流程）、`digest.write_outputs`（落盘产物并刷新 telemetry/progress）。
- 诊断入口：`digest.inspect_cache`（缓存命中/失效统计与逐项查看）、`digest.validate_links`（按需执行链接校验）、`digest.summarize_progress`（读取 `.monitoring` 增量）、`digest.list_outputs`（汇总当前生成的产物清单）。
- 运行态辅助：`digest.describe_run_config`（解出模型与并发配置）、`digest.reset_run_state`（清理缓存与模型偏好）、`digest.available_prompts`（列出可用提示模版），帮助调用方在执行昂贵操作前预判副作用。
- 跨工具复用：`telemetry.report_metrics`、`progress.watch` 等通用 MCP 工具暴露统一指标/进度接口，digest 相关微工具与其他工作流共同复用。
- 每个工具保持单一职责，运行时通过 `DigestRuntimeRegistry` 共享状态与互斥控制，降低巨型工具带来的锁粒度与启动成本。

- **T4.1** Deprioritized 更新 `fast_mcp_server.py`，引入 `DigestRuntimeRegistry` 缓存与依赖注入，集中管理 `DigestYAMLCache`、`DigestGenerationEngine`、`DigestIOManager` 等共享实例，并为各个微型工具提供生命周期钩子。  
  `参考: fast_mcp_server.py:148-188`
- **T4.2** Deprioritized将 `register_dynamic_resources()` 改为按需注册新的 digest 微工具，支持启动期最小化扫描且允许延迟装载诊断型命令。  
  `参考: fast_mcp_server.py:283-310`
- **T4.3** Deprioritized 为 `digest.*` 微工具与 `generate_github_pages` 引入统一的互斥与队列抽象，确保跨工具文件写入、缓存刷新与 telemetry 写入遵循同一调度策略。  
  `参考: fast_mcp_server.py:217-269`
- **T4.4** ✅ 在 MCP 层聚合各微工具的性能指标，输出 per-tool 视图并保留 digest 级别汇总，兼容现有 `.monitoring` 产物。  
  `参考: 1.0-architecture-review-webplatform-digest.md §11`



## 风险与依赖
- 需要评估引入新指标对日志量与存储的影响，避免额外的运维压力。
- MCP 运行时的改动可能影响现有工具调用方式，应提前与下游使用方沟通变更窗口。
- LLM 速率限制策略需与平台配额对齐，避免误触发 API 限制。

## 当前状态概览（2025-10-23）
- Telemetry 运行路径：通过 MCP server 调用工具时自动记录；Prometheus 指标在进程内注册（尚未开放 /metrics 端点）。
- 产物位置：
  - 事件：`.monitoring/webplatform-telemetry.jsonl`
  - 进度：`.monitoring/webplatform-progress.json`（默认 0.5s 节流，可用 `WEBPLATFORM_PROGRESS_MIN_INTERVAL` 配置）
- 并发治理：可通过环境变量调优并发与速率；出现连续失败将自动进入冷却期。（M2 已完成）
- Pipeline：已引入 YAML 内存缓存与磁盘回填；调试日志输出命中计数，后续将暴露到 Telemetry（T3.2 完成）。
- 待选增强：在 `fast_mcp_server.py` 暴露 `/metrics` 端点；提供 JSONL 基线汇总脚本（P95/P99、失败率、fallback 率）。

## 验收标准
- 指标平台可观测到 per-area 时长、LLM 调用次数、进度写入频率等核心数据。
- 在测试环境验证新版 pipeline 后，全量生成耗时与内存占用达到目标降幅。
- MCP 工具能够在高并发请求下保持稳定，未出现文件竞态或资源冲突。
