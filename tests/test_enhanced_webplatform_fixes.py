#!/usr/bin/env python3
"""
Test the fixes for enhanced webplatform digest tool based on sampling integration plan.
"""

import pytest

import sys
import asyncio
from pathlib import Path

# Add src to path
sys.path.insert(0, str(Path(__file__).parent.parent / 'src'))

from mcp_tools.enhanced_webplatform_digest import EnhancedWebplatformDigestTool


class MockSampleResult:
    """Mock result from FastMCP sampling."""
    def __init__(self, content_type='content'):
        if content_type == 'content':
            self.content = "# Test Digest\n\nThis is a test digest generated by sampling."
        elif content_type == 'text':
            self.text = "# Test Digest\n\nThis is a test digest with text attribute."
        else:
            # No content or text attribute to test fallback
            pass
    
    def __str__(self):
        return "# Test Digest\n\nFallback to str() conversion."


class MockContext:
    """Mock FastMCP context with sampling support."""
    
    def __init__(self, should_fail=False, content_type='content'):
        self.should_fail = should_fail
        self.content_type = content_type
        self.sample_called = False
        self.messages_param_used = False
    
    async def sample(self, messages=None, prompt=None, max_tokens=None, **kwargs):
        """Mock sampling method."""
        self.sample_called = True
        
        # Check if correct parameter is used
        if messages and isinstance(messages, list):
            self.messages_param_used = True
            if self.should_fail:
                raise Exception("Simulated sampling failure")
            return MockSampleResult(self.content_type)
        elif prompt:
            # Old incorrect parameter
            raise ValueError("Using deprecated 'prompt' parameter instead of 'messages'")
        else:
            raise ValueError("No valid parameters provided")


@pytest.mark.asyncio
async def test_prompt_loading():
    """Test 1: Verify prompt loading uses separate EN/ZH files."""
    print("\n=== Test 1: Prompt Loading ===")
    
    tool = EnhancedWebplatformDigestTool()
    ctx = MockContext()
    
    # Test EN prompt
    prompt_en = await tool._load_prompt(ctx, 'en', None, False)
    assert len(prompt_en) > 0, "EN prompt should load"
    print("‚úÖ EN prompt loaded successfully")
    
    # Test ZH prompt
    prompt_zh = await tool._load_prompt(ctx, 'zh', None, False)
    assert len(prompt_zh) > 0, "ZH prompt should load"
    print("‚úÖ ZH prompt loaded successfully")
    
    # Test bilingual defaults to EN (no bilingual file exists)
    prompt_bi = await tool._load_prompt(ctx, 'bilingual', None, False)
    assert len(prompt_bi) > 0, "Bilingual should default to EN"
    print("‚úÖ Bilingual mode defaults to EN (no bilingual file)")
    
    return True


@pytest.mark.asyncio
async def test_sampling_parameters():
    """Test 2: Verify sampling uses correct FastMCP parameters."""
    print("\n=== Test 2: Sampling Parameters ===")
    
    tool = EnhancedWebplatformDigestTool()
    ctx = MockContext()
    
    yaml_data = {
        'version': '139',
        'features': [
            {'title': 'Test Feature', 'link': 'http://example.com', 'description': 'Test'}
        ],
        'statistics': {'total_features': 1, 'total_links': 1}
    }
    
    # Generate digest with sampling
    digest = await tool._generate_digest_from_yaml(ctx, yaml_data, 'en', None, False)
    
    assert ctx.sample_called, "Sampling should be called"
    assert ctx.messages_param_used, "Should use 'messages' parameter, not 'prompt'"
    assert "Test Digest" in digest, "Should return sampled content"
    print("‚úÖ Sampling uses correct 'messages' parameter")
    print("‚úÖ Sampling response handled correctly")
    
    return True


@pytest.mark.asyncio
async def test_sampling_fallback():
    """Test 3: Verify error handling and fallback."""
    print("\n=== Test 3: Error Handling ===")
    
    tool = EnhancedWebplatformDigestTool()
    
    # Test with different response attributes
    for content_type in ['content', 'text', 'none']:
        ctx = MockContext(content_type=content_type)
        yaml_data = {
            'version': '139',
            'features': [{'title': 'Test', 'link': 'http://test.com'}],
            'statistics': {'total_features': 1}
        }
        
        digest = await tool._generate_digest_from_yaml(ctx, yaml_data, 'en', None, False)
        assert len(digest) > 0, f"Should handle {content_type} response type"
        print(f"‚úÖ Handles response with {content_type} attribute")
    
    # Test sampling failure with fallback
    ctx_fail = MockContext(should_fail=True)
    digest_fallback = await tool._generate_digest_from_yaml(ctx_fail, yaml_data, 'en', None, False)
    assert len(digest_fallback) > 0, "Should generate fallback digest on failure"
    assert "Chrome 139" in digest_fallback, "Fallback should contain version"
    print("‚úÖ Falls back to non-LLM digest on sampling failure")
    
    return True


@pytest.mark.asyncio
async def test_resource_loading():
    """Test 4: Verify resource loading doesn't raise NotImplementedError."""
    print("\n=== Test 4: Resource Loading ===")
    
    tool = EnhancedWebplatformDigestTool()
    ctx = MockContext()
    
    # This should not raise NotImplementedError anymore
    try:
        content = await tool._load_release_notes(ctx, '139', 'stable', None, False)
        # Content might be None if file doesn't exist, but shouldn't raise NotImplementedError
        print("‚úÖ Resource loading no longer raises NotImplementedError")
    except NotImplementedError:
        print("‚ùå NotImplementedError still present in resource loading")
        return False
    except Exception as e:
        # Other exceptions are OK (file not found, etc.)
        print(f"‚úÖ Resource loading handles errors gracefully: {type(e).__name__}")
    
    return True


@pytest.mark.asyncio
async def test_bilingual_generation():
    """Test 5: Verify bilingual mode generates two separate files."""
    print("\n=== Test 5: Bilingual Mode ===")
    
    tool = EnhancedWebplatformDigestTool()
    ctx = MockContext()
    
    # Note: This test verifies the logic structure, not actual file generation
    # The actual run method handles bilingual mode correctly at lines 91-110
    
    # We can verify the path generation for bilingual mode
    path_en = tool._get_digest_path('139', 'stable', None, 'en')
    path_zh = tool._get_digest_path('139', 'stable', None, 'zh')
    
    assert 'en' in str(path_en), "EN path should contain language marker"
    assert 'zh' in str(path_zh), "ZH path should contain language marker"
    assert path_en != path_zh, "EN and ZH should have different paths"
    
    print("‚úÖ Bilingual mode would generate separate EN and ZH files")
    print(f"   EN path: {path_en}")
    print(f"   ZH path: {path_zh}")
    
    return True


async def main():
    """Run all tests."""
    print("=" * 60)
    print("Testing Enhanced WebPlatform Digest Tool Fixes")
    print("Based on: sampling-integration-plan.md")
    print("=" * 60)
    
    tests = [
        ("Prompt Loading", test_prompt_loading),
        ("Sampling Parameters", test_sampling_parameters),
        ("Error Handling", test_sampling_fallback),
        ("Resource Loading", test_resource_loading),
        ("Bilingual Mode", test_bilingual_generation)
    ]
    
    results = []
    for name, test_func in tests:
        try:
            success = await test_func()
            results.append((name, success))
        except Exception as e:
            print(f"‚ùå Test '{name}' failed with exception: {e}")
            results.append((name, False))
    
    # Summary
    print("\n" + "=" * 60)
    print("TEST SUMMARY")
    print("=" * 60)
    
    total = len(results)
    passed = sum(1 for _, success in results if success)
    
    for name, success in results:
        status = "‚úÖ PASS" if success else "‚ùå FAIL"
        print(f"{status}: {name}")
    
    print(f"\nTotal: {passed}/{total} tests passed")
    
    if passed == total:
        print("\nüéâ All fixes verified successfully!")
        return 0
    else:
        print("\n‚ö†Ô∏è Some tests failed. Please review.")
        return 1


if __name__ == "__main__":
    exit_code = asyncio.run(main())
    exit(exit_code)
